{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Watson",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1XrNhEkCk8bdE_YqyTaIB74YrtKvV-IWs",
      "authorship_tag": "ABX9TyPEULA0E4nVloJXOMw6fSDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esalimi/Kaggle_NLI/blob/master/Watson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuXYW88r9mAs"
      },
      "source": [
        "#**Step 1: Building the BERT Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-BRa5GJ9d7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "ccc8447c-c534-46da-e0cd-b4615930a60f"
      },
      "source": [
        "!pip install keras-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=42badc02b48a28812e3a1c424b9d3c20d9eb3df1acff4372af1eba4931533da3\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=8e36dc3de66ddaa211cff8ff74b6e839f5b32d2ecdf07d37be8a58ee2ff553bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=9e63d64a0750766c5617bc200bbfd06f50ade55d0e79323ac81fc74589f5afec\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=03563de335e098dc843ebe2330a7063d689321bd8b088fbb3b3990cb79ad41df\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=30b41fd303a488985f9e5bc2c482ba01c8acfec1d3cc753fb3724239834471e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=7c3be785af3451d5a8af4fcd8f50d15bf9efb2d47ce6e1dc2a95dd899e188ade\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=0d6772add001526e3b3f913c903bbfe7129e1d2a76a5bbbe52a0e6bbb1564738\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=ff941ae2f835a2cb537e630780b20370d728606f92bcd8f81e59e7b0a6ea3197\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrJPcBKt-Dzn"
      },
      "source": [
        "# Importing required packages keras, keras_bert, os\n",
        "import os\n",
        "import json\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "# Importing required packages from keras_bert\n",
        "from keras_bert.loader import load_trained_model_from_checkpoint\n",
        "from keras_bert.bert import *\n",
        "from keras_bert import extract_embeddings "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3T8NwaY-Ij0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "42778556-dba5-49c0-a20f-42a63d507c72"
      },
      "source": [
        "# Loading bert_base from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_path = '/content/drive/My Drive/uncased_L-12_H-768_A-12'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDEF1qAZ-fBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "8144b2e4-7c99-400b-cf04-135f40f73eb3"
      },
      "source": [
        "# Testing the import\n",
        "texts = ['all work and play', 'makes jack a dull boy~']\n",
        "embeddings = extract_embeddings(model_path, texts)\n",
        "embedding_sentence1 = embeddings[0]\n",
        "embedding_sentence2 = embeddings[1]\n",
        "\n",
        "print(len(embedding_sentence1[0]))\n",
        "print(embeddings[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768\n",
            "[[-0.3183194   0.17749757  0.14557648 ... -0.47990972 -0.685876\n",
            "  -0.14804517]\n",
            " [-0.2539365   0.20985767  0.24948463 ... -0.24461871 -0.4896369\n",
            "  -0.24464582]\n",
            " [ 0.08667843  0.17276856  0.31595454 ... -0.75597656 -0.7137721\n",
            "  -0.20142424]\n",
            " [ 0.1849249   0.46440715 -0.4347621  ...  0.4690967   0.11448962\n",
            "  -0.4629252 ]\n",
            " [ 0.25327638 -0.06883571 -0.16352269 ... -1.0589056  -0.70101875\n",
            "   0.05821889]\n",
            " [-0.06629732  1.3095123   0.25281972 ... -1.1509613  -0.30832586\n",
            "  -0.14396843]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuWhK3BBAkkR"
      },
      "source": [
        "# Function to define the structure of the bert\n",
        "\n",
        "def get_bert_model(model_path, trainable = True, max_tokens=512, mode = 'pair'):\n",
        "  config_file = os.path.join(model_path, 'bert_config.json')\n",
        "  checkpoint_file = os.path.join(model_path, 'bert_model.ckpt')\n",
        "  bert = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=False, trainable=True)\n",
        "  \"\"\"\n",
        "  The pair mode gets 2 sntcs as input: premise & hypothesis.\n",
        "  Using Lambda layers: Lambda layer is an easy way to customize a layer to do simple arithmetic\n",
        "  \"\"\"\n",
        "  if mode == 'pair':\n",
        "    input_tensor = layers.Input(shape=(2,max_tokens, ))\n",
        "    word_tokens = layers.Lambda(lambda x: x[:,0, :])(input_tensor)\n",
        "    segment_encode = layers.Lambda(lambda x: x[:,1,:])(input_tensor)\n",
        "  if mode == 'single':\n",
        "    input_tensor = layers.Input(shape=(max_tokens, ))\n",
        "    word_tokens = input_tensor\n",
        "    segment_encode = layers.Lambda(lambda x: x*0)(input_tensor) #seems\n",
        "  \n",
        "  output0 = bert([word_tokens, segment_encode])\n",
        "  output1 = layers.Lambda(lambda x: x[:,0,:])(output0) #0-> cls : [cls]+...[SEP],...,[sep]\n",
        "  output = layers.Dense(768)(output1)\n",
        "  final_output = layers.Dense(3, activation='softmax')(output)\n",
        "\n",
        "  model = keras.models.Model(inputs =input_tensor , outputs =final_output )\n",
        "  print(model.summary)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3N4j-aOA2WX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "1acc30da-a8d7-4d28-eb09-5dd6f5e05200"
      },
      "source": [
        "# Test for model installation: calling the function to print model summary\n",
        "model = get_bert_model(model_path)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Model.summary of <tensorflow.python.keras.engine.functional.Functional object at 0x7f5bc80b0748>>\n",
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 512)          0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 512)          0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "functional_7 (Functional)       (None, 512, 768)     108891648   lambda[0][0]                     \n",
            "                                                                 lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 768)          0           functional_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            2307        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 109,484,547\n",
            "Trainable params: 109,484,547\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0S45KPCBQZb"
      },
      "source": [
        "# **Step 2: Importing Data from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbFoj-WKA5Nw"
      },
      "source": [
        "# Importing required packages\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqyDmVXYBXdj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "add3991e-0cea-4939-dc62-d1a45f1af435"
      },
      "source": [
        "# reading the training data from google drive: English senteces so far\n",
        "data =  pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "data.head()\n",
        "\n",
        "# Picking only english sentences\n",
        "data_en = data[data['language']=='English']\n",
        "data_en.head()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5130fd2cb5</td>\n",
              "      <td>and these comments were considered in formulat...</td>\n",
              "      <td>The rules developed in the interim were put to...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5b72532a0b</td>\n",
              "      <td>These are issues that we wrestle with in pract...</td>\n",
              "      <td>Practice groups are not permitted to work on t...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5622f0c60b</td>\n",
              "      <td>you know they can't really defend themselves l...</td>\n",
              "      <td>They can't defend themselves because of their ...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>fdcd1bd867</td>\n",
              "      <td>From Cockpit Country to St. Ann's Bay</td>\n",
              "      <td>From St. Ann's Bay to Cockpit Country.</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7cfb3d272c</td>\n",
              "      <td>Look, it's your skin, but you're going to be i...</td>\n",
              "      <td>The boss will fire you if he sees you slacking...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... label\n",
              "0  5130fd2cb5  ...     0\n",
              "1  5b72532a0b  ...     2\n",
              "3  5622f0c60b  ...     0\n",
              "7  fdcd1bd867  ...     2\n",
              "8  7cfb3d272c  ...     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AfF8vlHB4L4"
      },
      "source": [
        "# Training data set: Input and labels\n",
        "X_train = data_en[['premise','hypothesis']]\n",
        "labels = data_en['label']\n",
        "\n",
        "#Need to convert labels (0,1,2) into binary labels\n",
        "LB = preprocessing.LabelBinarizer()\n",
        "y_train = LB.fit_transform(labels)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M5SvzEeCWw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1491a56-f9ef-4eee-a5db-c0bb1ffcb2d3"
      },
      "source": [
        "# Printing data to verify the import process\n",
        "X_train.head()\n",
        "y_train[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [0, 1, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 1, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXXq41ZzCheO"
      },
      "source": [
        "# **Step 3: Fine Tuning Bert**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OduFFj2fCae2"
      },
      "source": [
        "import numpy as np\n",
        "import codecs\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "token_dict = {}\n",
        "with codecs.open('/content/drive/My Drive/uncased_L-12_H-768_A-12/vocab.txt', 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0ncJ41ICmSk"
      },
      "source": [
        "from keras_bert import Tokenizer\n",
        "tokenizer = Tokenizer(token_dict)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmDw4zsODlxp"
      },
      "source": [
        "def bertEncoder(sentences_df,max_len =512):\n",
        "  \"\"\"\n",
        "  inputs: a dataframe of form df[sentence1,sentence2]\n",
        "  output : bert encoded in the form of [indeces,segments]\n",
        "  Note: for pretraining you will need Mask input as well.\n",
        "  \"\"\"\n",
        "  encoded_data = []\n",
        "  for idx, row in sentences_df.iterrows():\n",
        "    indices, segments = tokenizer.encode(first=row[0], second=row[1], max_len=512)\n",
        "    encoded_data.append([indices,segments])\n",
        "    # encode_seg.append(segments)\n",
        "  return np.array(encoded_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLQp_MmcEaSo"
      },
      "source": [
        "# **Step 4: Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0wnHcS4ESKV"
      },
      "source": [
        "train_data= bertEncoder(X_train, max_len=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbESLHaUEubw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6102a802-0370-416e-b006-b141d64f12db"
      },
      "source": [
        "len(train_data[0])\n",
        "np.shape(train_data),np.shape(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6870, 2, 512), (6870, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF1eT04XExOe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}